{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from os import listdir\n",
    "from os.path import basename, splitext, exists, join\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CROP FACE DETECTION\n",
    "\n",
    "# load OpenCV face detector\n",
    "face_cascade = cv2.CascadeClassifier('opencv-files/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "\n",
    "def face_detection(dataset_path, folder_name):\n",
    "    k = 0\n",
    "    images_list = []\n",
    "    images_label = []\n",
    "    labels_name = []\n",
    "    for root in listdir(dataset_path):\n",
    "        if (not root.startswith('.')):\n",
    "            label_dir = join(dataset_path, root)\n",
    "            for img_paths in glob.glob(os.path.join(label_dir, \"*\")):\n",
    "                img = cv2.imread(img_paths)\n",
    "                img_name = os.path.basename(img_paths) \n",
    "                img_name = os.path.basename(img_paths) \n",
    "                frame = cv2.resize(img, (640, 480))\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)\n",
    "                for (x, y, w, h) in faces:\n",
    "                    face_crop = frame[y:y+h,x:x+w]  \n",
    "                    os.makedirs(folder_name + '/' + root, exist_ok=True)\n",
    "                    cv2.imwrite(folder_name + '/' + root + '/' +  img_name, face_crop)  \n",
    "            k =+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping faces from the images train folder\n",
    "DATASET_TRAIN_PATH = 'data/train'\n",
    "FOLDER_TRAIN_NAME = 'data/train_face'\n",
    "os.makedirs(FOLDER_TRAIN_NAME, exist_ok=True)\n",
    "face_detection(DATASET_TRAIN_PATH, FOLDER_TRAIN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping faces from the images validation folder\n",
    "DATASET_VAL_PATH = 'data/val'\n",
    "FOLDER_VAL_NAME = 'data/val_face'\n",
    "os.makedirs(FOLDER_VAL_NAME, exist_ok=True)\n",
    "face_detection(DATASET_VAL_PATH, FOLDER_VAL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transforms\n",
    "data_transforms = {\n",
    "    'train_face': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_face': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_face', 'val_face']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train_face', 'val_face']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train_face', 'val_face']}\n",
    "class_names = image_datasets['train_face'].classes\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD_NAME = 'RESNET18' \n",
    "model = None\n",
    "if METHOD_NAME == 'RESNET18':\n",
    "    ## resnet18\n",
    "    model = models.resnet18(pretrained=True)\n",
    "if METHOD_NAME == 'DENSENET161':\n",
    "    # densenet161\n",
    "    model = models.densenet161(pretrained=True)\n",
    "if METHOD_NAME == 'ALEXNET':\n",
    "    ## alexnet\n",
    "    model = models.alexnet(pretrained=True) \n",
    "if METHOD_NAME == 'VGG16':\n",
    "    # vgg16\n",
    "    model =  models.vgg16(pretrained=True)\n",
    "\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************************\n",
    "# TRAINING THE MODEL\n",
    "#******************************************************************\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train_face', 'val_face']:\n",
    "            if phase == 'train_face':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train_face':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val_face' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train_face Loss: 6.0877 Acc: 0.0000\n",
      "val_face Loss: 3.2319 Acc: 0.0000\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train_face Loss: 1.7661 Acc: 0.0000\n",
      "val_face Loss: 2.6164 Acc: 0.0000\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train_face Loss: 1.1587 Acc: 0.0000\n",
      "val_face Loss: 1.6885 Acc: 0.0000\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train_face Loss: 0.9761 Acc: 0.0000\n",
      "val_face Loss: 1.4047 Acc: 0.0000\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train_face Loss: 0.6522 Acc: 0.0000\n",
      "val_face Loss: 1.2339 Acc: 0.0000\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train_face Loss: 0.8286 Acc: 0.0000\n",
      "val_face Loss: 0.8777 Acc: 0.0000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train_face Loss: 0.9910 Acc: 0.0000\n",
      "val_face Loss: 1.4715 Acc: 0.0000\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train_face Loss: 1.0899 Acc: 0.0000\n",
      "val_face Loss: 1.3195 Acc: 0.0000\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train_face Loss: 0.6441 Acc: 0.0000\n",
      "val_face Loss: 1.1747 Acc: 0.0000\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train_face Loss: 0.3488 Acc: 0.0000\n",
      "val_face Loss: 0.9919 Acc: 0.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train_face Loss: 0.3653 Acc: 0.0000\n",
      "val_face Loss: 0.9191 Acc: 0.0000\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train_face Loss: 0.5291 Acc: 0.0000\n",
      "val_face Loss: 0.8116 Acc: 0.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train_face Loss: 0.3989 Acc: 0.0000\n",
      "val_face Loss: 0.9491 Acc: 0.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train_face Loss: 0.5776 Acc: 0.0000\n",
      "val_face Loss: 0.8599 Acc: 0.0000\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train_face Loss: 0.5914 Acc: 0.0000\n",
      "val_face Loss: 0.8834 Acc: 0.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train_face Loss: 0.4591 Acc: 0.0000\n",
      "val_face Loss: 0.8460 Acc: 0.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train_face Loss: 0.7698 Acc: 0.0000\n",
      "val_face Loss: 0.8945 Acc: 0.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train_face Loss: 0.5541 Acc: 0.0000\n",
      "val_face Loss: 0.9432 Acc: 0.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train_face Loss: 0.5953 Acc: 0.0000\n",
      "val_face Loss: 0.8103 Acc: 0.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train_face Loss: 0.5407 Acc: 0.0000\n",
      "val_face Loss: 0.8679 Acc: 0.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train_face Loss: 0.4676 Acc: 0.0000\n",
      "val_face Loss: 0.9789 Acc: 0.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train_face Loss: 0.5127 Acc: 0.0000\n",
      "val_face Loss: 0.9616 Acc: 0.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train_face Loss: 0.4632 Acc: 0.0000\n",
      "val_face Loss: 0.8616 Acc: 0.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train_face Loss: 0.6144 Acc: 0.0000\n",
      "val_face Loss: 0.9193 Acc: 0.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train_face Loss: 0.6079 Acc: 0.0000\n",
      "val_face Loss: 0.8536 Acc: 0.0000\n",
      "\n",
      "Training complete in 0m 22s\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('model', exist_ok=True)\n",
    "torch.save(model, 'model/mymodel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
